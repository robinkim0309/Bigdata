## 3.3 디자인 패턴 7 : 앙상블

앙상블은 단일 머신러닝 모델의 오류(편향과 분산)를 줄이기 위해 여러 머신러닝 모델을 결합하여 전체 모델 성능을 향상 시키는 기술

* * *

### 3.3.1 문제

신생아 체중 예측 모델의 특징을 가공하고 계층을 추가하여 모델을 학습한 결과 학습 데이터 셋에 대한 예측 오류가 거의 0 이 되었다. 훌륭하다!
그러나 실제 병원에서 이 모델을 사용하려고 테스트 셋으로 평가를 해보니 모델의 예측이 모두 잘못되었다. 어떻게 된 것일까? 또 이 모델을 어떻게 고칠 수 있을까?

완벽한 머신러닝 모델은 없다. 머신 러닝의 오류는 아래와 같이 세 가지로 나눌 수 있다.

1. 줄일 수 없는 오류 - 데이터셋의 노이즈, 측정오류, 잘못된 학습 데이터
2. 편향으로 인한 오류 - 특징과 라벨 간의 관계에 대해 단순화하여 충분히 학습할 수 없게 만드는 요소 (과소적합)
3. 분산으로 인한 오류 - 학습 데이터에 대해 과도하게 학습하여 보이지 않는 새로운 예에 대해 일반화 할 수 없게 만드는 요소 (과대적합)

편향과 분산은 줄일 수 있는 오류이나, 트레이드오프의 특성을 가지고 있어서 모두 달성하기는 어렵다.
대규모 신경망과 같은 최신 머신러닝 기술을 사용할 경우, 규모가 아주 큰 모델의 경우 보간 임계값(interpolation threshold) 를 이용하여 학습 데이터와 새로운 데이터에 대한 오류를 모두 줄일 수 있다.(https://oreil.ly/PxUvs)

중소 규모의 문제에서 편향-분산 트레이드 오프를 줄이는 방법은?

* * *

### 3.3.2 솔루션

편향과 분산을 줄이고 모델 성능을 개선하기 위해 여러 머신러닝 모델을 결합하는 앙상블 방법이 있다. 배깅, 부스팅, 스태킹과 같은 일반적인 앙상블 방법을 알아보자.

#### 배깅 (bagging, boostrap aggregating 의 약자)

- 병령 앙상블 방법 중 하나로, 머신러닝 모델의 높은 분산을 해결하기 위해 사용된다.
boostrap 은 앙상블의 k 개의 하위 모델을 학습하는 데 사용되는 k 개의 개별 데이터 셋을 나타낸다.
개별 데이터셋은 원래 학습 데이터에서 무작위 샘플링(복원 추출)하는 방법으로 생성한다. 각 데이터 셋에는 일부 예제가 누락되거나 중복될 수 있다.
여러 모델의 학습이 끝나면 여러 앙상블 모델의 출력을 집계하는 과정이 이어진다. 회기 모델의 경우 출력의 평균값을 사용하고, 분류 모델의 경우 과반수 클래스를 사용한다.

배깅의 좋은 예로 랜덤포레스트를 들 수 있다. 전체 학습 데이터 셋을 무작위로 샘플링한 하위 데이터셋에서 여러 결정 트리를 학습 시킨 후 각 모델의 예측 값들을 집계하여 최종 예측값을 생성한다.
이로 인해 머신러닝 모델 출력의 분산을 줄일 수 있다.

![randomforest](./img/image2022-1-6_11-22-39.png)


```python
from sklearn.ensemble import RandomForestRegressor

# 50개의 트리로 모델 생성
RF_model = RandomForestRegressor(n_estimators=50,
                                 max_features='sqrt',
                                 n_jobs=-1, verbose = 1)

# 학습 데이터로부터 학습 시작
RF_model.fit(X_train, Y_train)
```

배깅에서 볼 수 잇는 모델 평균화는 모델의 분산을 줄이기 위해 신뢰할 수 있는 강력한 방법이다.
배깅을 사용할 때는 모델과 알고리즘이 동일하지만(랜덤 포레스트의 경우 하위 모델은 모두 의사결정트리) 
다른 앙상블 방법에서는 다른 방식으로 하위 모델을 결합하며 때로는 서로 다른 모델, 알고리즘, 목적 함수를 사용할 수도 있다.


#### 부스팅 (boosting)

- 부스팅은 분산보다 편향을 줄이는 효과적인 앙상블 기술이며, 이전의 모델이 잘못 학습한 학습 데이터를 후속 모델이 제대로 학습할 수 있도록 앙상블 모델을 반복적으로 구축한다.

부스팅은 모델 간 팀워크가 이루어진다. 
처음 모델이 예측을 하면 그 예측 결과에 따라 데이터에 가중치가 부여되고, 부여된 가중치가 다음 모델에 영향을 준다. 
잘못 분류된 데이터에 집중하여 새로운 분류 규칙을 만드는 단계를 반복한다.
반복 횟수가 많아질수록 잔차가 0에 가까워지며, 원래 학습 데이터셋에 대한 예측은 점점 더 좋아진다. 

![boosting](./img/image2022-1-6_11-53-42.png)

AdaBoost, Gradient Boosing Machine, XGBoost 등이 있으며, Scikit-learn 과 tensorflow 등에 사용하기 쉽게 구현되어 있다.

```python 
from sklearn.ensemble import GradientBoostingRegressor

# GradientBoosting 회귀 분석기 생성
GB_model = GradientBoostingRegressor(n_estimators=1,
                                     max_depth=1,
                                     learning_rate=1, 
                                     criterion= 'mse')

# 학습 데이터로부터 학습 시작
GB_model.fit(X_train, Y_train) 
```


#### bagging 과 boosting의 앙상블 구조적 차이

- bagging 은 모델이 병렬로 학습하는 반면 boosting 은 순차적으로 학습한다. 이전 모델의 오류를 개선하기 위해 후속 모델에 변화된 가중치를 적용하여 오답에 대해서 더 높은 가중치를 준다. (오답 노트 복기)
- boosing 은 학습 속도가 느리고 오버피팅 우려가 있음
- 개별 트리의 낮은 성능이 문제라면 정답에 가까워지는 boosting
- 오비피팅이 문제라면 일반화하는 bagging

![structure](./img/image2022-1-6_11-56-50.png)


#### 스태킹(stacking)

초기 모델들은 전체 학습 데이터셋을 학습한다. 초기 모델의 출력을 특징으로 하여 2차 메타 모델을 학습 시킨다. 2차 메타 모델은 초기 모델의 결과를 가장 잘 학습하여 학습 오류를 줄이는 방법을 학습하며, 모든 유형의 머신러닝 모델을 적용할 수 있다.

![stacking](./img/image2022-1-6_13-8-35.png)


```python 
members = [model_1, model_2, model_3] # 학습된 초기 모델들을 포함하는 리스트

# fit and save models
n_members = len(members)

for i in range(n_members):
    # fit model
    model = fit_model(members[i])
    # save model
    filename = 'models/model_' + str(i + 1) + '.h5'
    model.save(filename, save_format='tf')
    print('Saved {}\n'.format(filename))
```

하위 모델은 개별 입력으로서 더 큰 스태킹 앙상블 모델에 통합된다. 이 때 pre-trained 모델의 가중치를 더 이상 학습하지 않도록 고정시킨다.

```python
for i in range(n_members):
    model = members[i]
    for layer in model.layers:
        # make not trainable
        layer.trainable = False
        # rename to avoid 'unique layer name' issue
        layer._name = 'ensemble_' + str(i+1) + '_' + layer.name
```

구성 요소를 연결하는 앙상블 모델을 만든다.

```python
member_inputs = [model.input for model in members]

# concatenate merge output from each model
member_outputs = [model.output for model in members]
merge = layers.concatenate(member_outputs)
h1 = layers.Dense(30, activation='relu')(merge)
h2 = layers.Dense(20, activation='relu')(h1)
h3 = layers.Dense(10, activation='relu')(h2)
h4 = layers.Dense(5, activation='relu')(h2)
ensemble_output = layers.Dense(1, activation='relu')(h3)
ensemble_model = Model(inputs=member_inputs, outputs=ensemble_output)

# plot graph of ensemble
tf.keras.utils.plot_model(ensemble_model, show_shapes=True, to_file='ensemble_graph.png')

# compile
ensemble_model.compile(loss='mse', optimizer='adam', metrics=['mse'])
```
2차 모델은 2개의 fully connected hidden layer 로 이루어진 신경망이다. 이 네트워크는 앙상블 멤버의 결과를 가장 잘 결합하여 예측하는 방법을 학습한다.

* * *

### 3.3.3 작동 원리

1. 배깅 
   - 모델 평균화 방법 (군중의 지혜, 각 개별 모델이 임의의 양만큼 오류들을 가지고 있고 그 결과를 평균화 하면 임의의 오류가 상쇄되고 예측이 정답에 더 가까워진다.)
   - 평균적으로 앙상블은 적어도 개별 모델 이상의 성능을 발휘한다. 모델의 오류들이 완벽한 상관관계를 가질 경우(유사한 모델로만 구성될 경우) 모델 평균화는 전혀 도움이 되지 않음. 배깅의 성공 열쇠는 모델의 다양성이다.

2. 부스팅 
   - 각 반복 단계에서 잔차에 따라 모델에 패널티를 가하는 방식으로 작동한다. 점점 더 앙상블 모델은 예측하기 어려운 예제를 더 잘 예측할 수 있도록 개선된다.
   - 결과적으로 생성되는 앙상블 모델은 점점 더 복잡해지고 더 많은 용량을 가지게 된다. 편향은 모델의 과소적합 경향과 관련이 있는데, 부스팅은 예측하기 어려운 예제에 반복적으로 초점을 맞춤으로써 모델의 편향을 효과적으로 감소시킨다.

3. 스태킹 
   - 배깅과 부스팅의 장점을 결합한 것으로 2차 모델을 통해 모델 평균화보다 정교한 버전이 된다.
   - 학습 데이터셋에서 k 개의 모델을 학습 시킨 후 결과를 평균하여 예측을 결정하는 단순 모델 평균화의 확장으로 볼 수 있다.

![stacking_1](./img/stacking1.PNG)

- 단순 모델 평균화는 배깅과 유사하지만, 배깅에서는 앙상블의 개별 모델이 모두 동일한 반면, 단순모델 평균화에서는 서로 다른 유형일 수 있다. 더 일반적으로는 가중 평균을 취하도록 평균화 단계를 수정할 수 있다. (정확도에 따라 특정 앙상블 모델에 더 많은 가중치를 부여할 수 있다)

![stacking_1_2](./img/stacking1_2.PNG)

- 스태킹은 모델 평균화의 고급버전으로 평균 또는 가중 평균을 취하는 대신, 두 번째 머신러닝 모델을 학습시켜 앙상블의 각 모델 결과를 가장 잘 결합하는 방법을 학습하게 하고 이를 통해 예측 값을 생성한다. 배깅 기술과 마찬가지로 분산 감소의 모든 이점을 취할 뿐 아니라 높은 편향을 제어한다.

![stacking_2](./img/stacking2.PNG)

* * *

### 3.3.4 트레이드 오프와 대안

#### 늘어난 학습 시간과 설계 시간
- 스태킹 앙상블 모델의 경우 전문 지식이 요구되고 의사 결정 사항들이 존재한다.
  - ML 아키텍처 재사용? 다양성? k 개 모델 개발, 유지관리 및 추론의 복잡성, 리소스 사용량 증가, 모델 개발하는 비용 시간 증가
- 앙상블을 채택하면서 증가한 비용과 시간을 잘 따젹보고 그만한 가치가 있는 지 고려해야 한다.
  - 정확성과 리소스 사용량을 선형 모델 또는 DNN 모델과 비교해보자. 신경망 앙상블을 증류(distilling) 해서 복잡성을 줄이고 성능을 향상 시킬 수도 있다.

#### 드롭아웃을 통한 배깅
- 딥러닝 정규화 기술이지만 배깅의 대체제로 간주할 수 있다. 신경망에서 학습의 각 미니 배치에 대해 네트워크의 뉴런을 무작위로 끄는 방법으로 기하급수적으로 많은 신경망의 배깅 앙상블을 결합할 수 있다.

![dropout](./img/image2022-1-6_13-0-15.png)

- 기존 배깅은 하위 모델이 독립적이나, 드롭아웃으로 학습할 때는 모델이 파라미터를 공유한다. 또한 각 학습 데이터셋에서 수렴할 때까지 학습하지 않고 학습 루프의 각 반복 시 서로 다른 노드가 드롭 아웃 되도록 하여 한번의 학습 단계만 거친다.


#### 모델 해석 가능성의 감소
- 딥러닝 모델이 예측을 수행하는 이유를 설명하기 어려운 것과 동일. 모델이 복잡해질 수록 해석 가능성이 낮아진다.
- 결정트리는 각 특징의 경계값으로 예측 로직을 설명할 수 있으나 랜덤 포레스트는 해석할 수 없다.

#### 문제에 맞는 도구 선택하기
- 앙상블 방법을 사용한다고 해서 반드시 성능 향상이 되지는 않는다. 잘못 사용하면 시간과 비용이 불필요하게 늘어날 수 있다.
- 부스팅 - 높은 편향 감소(과소 적합 시)
- 배깅 - 높은 분산 감소 (과대 적합시, 상관관계가 높은 오류를 가지는 모델들을 결합하는 것은 도움이 안됨)


#### 기타 앙상블 방법
- 베이지안 접근 방식을 통합, 구글의 AdaNet, AutoML (신경망 아키텍처 검색 + 강화학습)
